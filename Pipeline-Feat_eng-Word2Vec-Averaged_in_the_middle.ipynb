{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\manon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open (file) as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "    id_name = file.split(\"/\")[-1].split(\".txt\")[0]\n",
    "    id_name = id_name.split(\"\\\\\")[-1]\n",
    "    return id_name,lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_repetition(lines):\n",
    "    new_list =[]\n",
    "    for line in lines:\n",
    "        if \"SYSTEM\" in line:\n",
    "            new_list.append(line.strip())\n",
    "    #create dictionary and count the repetition of sentence (Feature 1)\n",
    "    new_dict = {}\n",
    "    for li in new_list:\n",
    "        if li in new_dict:\n",
    "            new_dict[li] += 1\n",
    "        else:\n",
    "            new_dict[li] = 1\n",
    "    num_rep = sum([value for value in new_dict.values() if value > 1])\n",
    "    #Percentage of the repetition sentence in the entire conversation (Feature 3)\n",
    "    num_rep_per = num_rep/len(new_list)\n",
    "    \n",
    "    return num_rep,num_rep_per, len(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_conv(file):\n",
    "    # get everything\n",
    "    \n",
    "    dict_conv_only = create_user_conv(file)\n",
    "    compound_score, tot_pos_sen, tot_neg_sen = get_sentiment(dict_conv_only)\n",
    "    return compound_score, tot_pos_sen, tot_neg_sen\n",
    "    \n",
    "    \n",
    "def get_sentiment(dict_conv_only):\n",
    "    # generate the compound sentiment score and store in conversation\n",
    "\n",
    "    for sentence in dict_conv_only.keys():\n",
    "        compound_sentence = generatesentiment(sentence)\n",
    "        # add compound score to sentence \n",
    "        dict_conv_only[sentence] = compound_sentence\n",
    "    # add total compound score of sentence to conversation\n",
    "    compound_score = sum(dict_conv_only.values())/len(dict_conv_only.values())\n",
    "    tot_pos_sen = len([x for x in dict_conv_only.values() if x > 0])/len(dict_conv_only.values())\n",
    "    tot_neg_sen = len([x for x in dict_conv_only.values() if x < 0])/len(dict_conv_only.values())\n",
    "    return compound_score, tot_pos_sen, tot_neg_sen\n",
    "    \n",
    "    \n",
    "def create_user_conv(lines):\n",
    "    #create a list per conversation of only USER sentences \n",
    "    # and 'thankyou goodbye left out'\n",
    "    conv_only_user = []\n",
    "    new_list =[]\n",
    "    for line in lines:\n",
    "        # only USER input\n",
    "        if \"USER\" in line:\n",
    "            # Remove the word '[USER]'\n",
    "            line = line.replace(\"[USER]\", \"\")\n",
    "            line = line.strip()\n",
    "            new_list.append(line)\n",
    "    # Remove 'thank you good bye' if in last sentences\n",
    "    for i,j in dic.items():\n",
    "        new_list[-1] = new_list[-1].replace(i, j)\n",
    "        new_list[-2] = new_list[-2].replace(i, j)\n",
    "    conv_only_user.append(new_list)\n",
    "\n",
    "    # convert to dict \n",
    "    # with every conversation a dict with sentences as keys\n",
    "    # and compound pos neg score (as 0)\n",
    "    for conv in conv_only_user:\n",
    "        new_dict2 = {}\n",
    "        for sentence in conv:\n",
    "            new_dict2[sentence] = 0\n",
    "    return new_dict2       \n",
    "        \n",
    "def generatesentiment(sentence):\n",
    "    # function for sentiment analysis on sentences\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    return ss['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repetition of the chatbot / 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "text_files_dsat = glob.glob(\"./allconversations/dsat/*.txt\")\n",
    "text_files_sat = glob.glob(\"./allconversations/sat/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences to delete\n",
    "dic = {'thankyou':\"\", 'thank you':\"\", \"goodbye\":\"\", \"good bye\":\"\", 'bye':\"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing for (google) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's Word2Vec model\n",
    "model = KeyedVectors.load_word2vec_format('./google_model/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\"user\")\n",
    "stop_words.append(\"system\")\n",
    "stop_words = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Helper function to tokenize a string\n",
    "def tokenize_sentence(sentence):\n",
    "    line = []\n",
    "    sentence = sentence.strip()\n",
    "    split = str(sentence).lower().translate(translator).split()\n",
    "    for word in split:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word not in model.vocab:\n",
    "            continue\n",
    "        line.append(word) \n",
    "    return line\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Word2Vec score per sentence (tokenized words in list)            \n",
    "def get_sentence_W2V_score(sentence):\n",
    "    sentence_score = []\n",
    "    for word in sentence:\n",
    "        if word == '':\n",
    "            continue\n",
    "        word_score =(model[word])\n",
    "        sentence_score.append(word_score)\n",
    "   \n",
    "    sentence_score = np.array(sentence_score)\n",
    "    sentence_score = np.mean(sentence_score)\n",
    "    return sentence_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_conversation_w2v_score(lines):  \n",
    "    conversation_score = []\n",
    "    for line in lines:\n",
    "        line = tokenize_sentence(line)\n",
    "        sentence_score = get_sentence_W2V_score(line)\n",
    "        conversation_score.append(sentence_score)\n",
    "    conversation_score = np.array(conversation_score)\n",
    "    conversation_score = conversation_score[~np.isnan(conversation_score)]\n",
    "    avg_conv_score = np.mean(conversation_score, axis=0)\n",
    "    sum_conv_score = np.sum(conversation_score, axis=0)\n",
    "    return avg_conv_score, sum_conv_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id','num_rep','num_rep_per','len_conversation','total_compound_conv', 'tot_pos_sen', 'tot_neg_sen', 'avg_w2v_score', 'sum_w2v_score', 'Is_satisfied']\n",
    "df_rep_dsat = pd.DataFrame(index=range(len(text_files_dsat)), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,file in enumerate(text_files_dsat):\n",
    "    id_number,lines = read_file(file)\n",
    "    num_rep, num_rep_per,len_conversation = number_repetition(lines)\n",
    "    compound_score, tot_pos_sen, tot_neg_sen = pos_neg_conv(lines)\n",
    "    avg_w2v_score, sum_w2v_score  = generate_conversation_w2v_score(lines)\n",
    "    df_rep_dsat.iloc[idx] = pd.Series({'id':id_number, 'num_rep':num_rep, 'num_rep_per':num_rep_per,\n",
    "                                      'len_conversation':len_conversation,\n",
    "                                       'total_compound_conv':compound_score, 'tot_pos_sen':tot_pos_sen, \n",
    "                                       'tot_neg_sen':tot_neg_sen, 'avg_w2v_score': avg_w2v_score,\n",
    "                                       'sum_w2v_score': sum_w2v_score, \n",
    "                                       'Is_satisfied':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_rep</th>\n",
       "      <th>num_rep_per</th>\n",
       "      <th>len_conversation</th>\n",
       "      <th>total_compound_conv</th>\n",
       "      <th>tot_pos_sen</th>\n",
       "      <th>tot_neg_sen</th>\n",
       "      <th>avg_w2v_score</th>\n",
       "      <th>sum_w2v_score</th>\n",
       "      <th>Is_satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voip-0241bbae39-20130327_195830</td>\n",
       "      <td>7</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0029501</td>\n",
       "      <td>-0.0619521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>voip-0f41c16f2f-20130325_192310</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12</td>\n",
       "      <td>0.10275</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.00621081</td>\n",
       "      <td>-0.068319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voip-0f41c16f2f-20130325_193723</td>\n",
       "      <td>4</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0463857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.0032949</td>\n",
       "      <td>-0.0461286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voip-0f41c16f2f-20130325_204340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00535927</td>\n",
       "      <td>-0.0535927</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voip-0f41c16f2f-20130402_005414</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00718453</td>\n",
       "      <td>-0.114952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id num_rep num_rep_per len_conversation  \\\n",
       "0  voip-0241bbae39-20130327_195830       7    0.583333               24   \n",
       "1  voip-0f41c16f2f-20130325_192310       4    0.666667               12   \n",
       "2  voip-0f41c16f2f-20130325_193723       4    0.571429               14   \n",
       "3  voip-0f41c16f2f-20130325_204340       0           0               10   \n",
       "4  voip-0f41c16f2f-20130402_005414       4         0.5               16   \n",
       "\n",
       "  total_compound_conv tot_pos_sen tot_neg_sen avg_w2v_score sum_w2v_score  \\\n",
       "0                   0           0           0    -0.0029501    -0.0619521   \n",
       "1             0.10275    0.333333    0.166667   -0.00621081     -0.068319   \n",
       "2           0.0463857    0.142857    0.142857    -0.0032949    -0.0461286   \n",
       "3                   0           0           0   -0.00535927    -0.0535927   \n",
       "4                   0           0           0   -0.00718453     -0.114952   \n",
       "\n",
       "  Is_satisfied  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rep_dsat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create feature dataframe for satisfied dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id','num_rep','num_rep_per','len_conversation','total_compound_conv', 'tot_pos_sen', 'tot_neg_sen', 'avg_w2v_score', 'sum_w2v_score',  'Is_satisfied']\n",
    "df_rep_sat = pd.DataFrame(index=range(len(text_files_sat)), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,file in enumerate(text_files_sat):\n",
    "    id_number,lines = read_file(file)\n",
    "    num_rep, num_rep_per,len_conversation = number_repetition(lines)\n",
    "    compound_score, tot_pos_sen, tot_neg_sen = pos_neg_conv(lines)\n",
    "    avg_w2v_score, sum_w2v_score = generate_conversation_w2v_score(lines)\n",
    "    df_rep_sat.iloc[idx] = pd.Series({'id':id_number, 'num_rep':num_rep, 'num_rep_per':num_rep_per,\n",
    "                                      'len_conversation':len_conversation,\n",
    "                                       'total_compound_conv':compound_score, 'tot_pos_sen':tot_pos_sen, \n",
    "                                       'tot_neg_sen':tot_neg_sen, 'avg_w2v_score': avg_w2v_score,\n",
    "                                       'sum_w2v_score': sum_w2v_score, 'Is_satisfied':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_rep</th>\n",
       "      <th>num_rep_per</th>\n",
       "      <th>len_conversation</th>\n",
       "      <th>total_compound_conv</th>\n",
       "      <th>tot_pos_sen</th>\n",
       "      <th>tot_neg_sen</th>\n",
       "      <th>avg_w2v_score</th>\n",
       "      <th>sum_w2v_score</th>\n",
       "      <th>Is_satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voip-00d76b791d-20130327_011609</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00802308</td>\n",
       "      <td>-0.1043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>voip-00d76b791d-20130327_012331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01544</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00458558</td>\n",
       "      <td>-0.0412702</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voip-0241bbae39-20130327_190942</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.0775</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.00563244</td>\n",
       "      <td>-0.0563244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voip-03c2655d43-20130327_194221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0257333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00702347</td>\n",
       "      <td>-0.0421408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voip-03c2655d43-20130327_194616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0257333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0080021</td>\n",
       "      <td>-0.0480126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id num_rep num_rep_per len_conversation  \\\n",
       "0  voip-00d76b791d-20130327_011609       0           0               14   \n",
       "1  voip-00d76b791d-20130327_012331       0           0               10   \n",
       "2  voip-0241bbae39-20130327_190942       0           0               10   \n",
       "3  voip-03c2655d43-20130327_194221       0           0                6   \n",
       "4  voip-03c2655d43-20130327_194616       0           0                6   \n",
       "\n",
       "  total_compound_conv tot_pos_sen tot_neg_sen avg_w2v_score sum_w2v_score  \\\n",
       "0              0.1003    0.285714           0   -0.00802308       -0.1043   \n",
       "1             0.01544         0.2           0   -0.00458558    -0.0412702   \n",
       "2             -0.0775           0         0.2   -0.00563244    -0.0563244   \n",
       "3           0.0257333    0.333333           0   -0.00702347    -0.0421408   \n",
       "4           0.0257333    0.333333           0    -0.0080021    -0.0480126   \n",
       "\n",
       "  Is_satisfied  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rep_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disatisfied files: 157\n",
      "Number of satisfied files: 314\n"
     ]
    }
   ],
   "source": [
    "print('Number of disatisfied files: {}'.format(len(df_rep_dsat)))\n",
    "print('Number of satisfied files: {}'.format(len(df_rep_sat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat 2 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df_rep_dsat,df_rep_sat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shuffle = new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"features_w2v.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.iloc[:,1:-1]\n",
    "y = new_df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 18]\n",
      " [ 4 76]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = classifier.predict_proba(X_test)\n",
    "y_pred_new2 = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sat_score\": 61.16}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dis_score = round(y_pred_new[0][0]*100,2)\n",
    "sat_score = round(y_pred_new[0][1]*100,2)\n",
    "result = {'sat_score': sat_score}\n",
    "print(json.dumps(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.81\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            PCC        p-value\n",
      "len_conversation__Is_satisfied        -0.353808   2.460829e-15\n",
      "len_conversation__tot_neg_sen          0.117055   1.100980e-02\n",
      "len_conversation__tot_pos_sen          0.066764   1.479761e-01\n",
      "len_conversation__total_compound_conv  0.141350   2.105281e-03\n",
      "num_rep__Is_satisfied                 -0.477934   2.981528e-28\n",
      "num_rep__len_conversation              0.858641  3.229487e-138\n",
      "num_rep__num_rep_per                   0.856732  5.911338e-137\n",
      "num_rep__tot_neg_sen                   0.083387   7.060020e-02\n",
      "num_rep__tot_pos_sen                   0.077346   9.361157e-02\n",
      "num_rep__total_compound_conv           0.168678   2.356180e-04\n",
      "num_rep_per__Is_satisfied             -0.522149   2.668523e-34\n",
      "num_rep_per__len_conversation          0.592383   5.972811e-46\n",
      "num_rep_per__tot_neg_sen               0.048848   2.900849e-01\n",
      "num_rep_per__tot_pos_sen               0.040176   3.843204e-01\n",
      "num_rep_per__total_compound_conv       0.172967   1.617397e-04\n",
      "tot_neg_sen__Is_satisfied             -0.022449   6.269947e-01\n",
      "tot_pos_sen__Is_satisfied              0.064181   1.643388e-01\n",
      "tot_pos_sen__tot_neg_sen              -0.203908   8.176718e-06\n",
      "total_compound_conv__Is_satisfied     -0.098092   3.330986e-02\n",
      "total_compound_conv__tot_neg_sen      -0.457296   1.022148e-25\n",
      "total_compound_conv__tot_pos_sen       0.788434  4.907513e-101\n"
     ]
    }
   ],
   "source": [
    "correlations = {}\n",
    "columns = df_shuffle.columns.tolist()\n",
    "columns.remove('id')\n",
    "\n",
    "\n",
    "for col_a, col_b in itertools.combinations(columns, 2):\n",
    "    correlations[col_a + '__' + col_b] = pearsonr(df_shuffle.loc[:, col_a], df_shuffle.loc[:, col_b])\n",
    "\n",
    "result = DataFrame.from_dict(correlations, orient='index')\n",
    "result.columns = ['PCC', 'p-value']\n",
    "\n",
    "print(result.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        PCC       p-value\n",
      "Is_satisfied__Is_satisfied         1.000000  0.000000e+00\n",
      "avg_w2v_score__Is_satisfied       -0.041356  3.705093e-01\n",
      "len_conversation__Is_satisfied    -0.353808  2.460829e-15\n",
      "num_rep__Is_satisfied             -0.477934  2.981528e-28\n",
      "num_rep_per__Is_satisfied         -0.522149  2.668523e-34\n",
      "sum_w2v_score__Is_satisfied        0.277596  8.820802e-10\n",
      "tot_neg_sen__Is_satisfied         -0.022449  6.269947e-01\n",
      "tot_pos_sen__Is_satisfied          0.064181  1.643388e-01\n",
      "total_compound_conv__Is_satisfied -0.098092  3.330986e-02\n"
     ]
    }
   ],
   "source": [
    "correlations = {}\n",
    "columns = new_df.columns.tolist()\n",
    "columns.remove('id')\n",
    "\n",
    "col_b ='Is_satisfied'\n",
    "\n",
    "for col_a in columns:\n",
    "    correlations[col_a + '__' + col_b] = pearsonr(new_df.loc[:, col_a], new_df.loc[:, col_b])\n",
    "\n",
    "result = DataFrame.from_dict(correlations, orient='index')\n",
    "result.columns = ['PCC', 'p-value']\n",
    "\n",
    "print(result.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_shuffle.iloc[:,:1]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of num_rep: 0.70\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of num_rep: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of num_rep_per: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_shuffle.iloc[:,1:2]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of num_rep_per: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of num_rep_exp: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_shuffle.iloc[:,2:3]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of num_rep_exp: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of len_conversation: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_shuffle.iloc[:,3:4]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of len_conversation: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of len_conversation: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Average and SUM word embedding\n",
    "X = new_df.iloc[:,7:9]\n",
    "y = new_df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of len_conversation: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_w2v_score</th>\n",
       "      <th>sum_w2v_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.0029501</td>\n",
       "      <td>-0.0619521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.00621081</td>\n",
       "      <td>-0.068319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0032949</td>\n",
       "      <td>-0.0461286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.00535927</td>\n",
       "      <td>-0.0535927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.00718453</td>\n",
       "      <td>-0.114952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.00606689</td>\n",
       "      <td>-0.0364014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.00656565</td>\n",
       "      <td>-0.229798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.0042346</td>\n",
       "      <td>-0.0804575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.00542563</td>\n",
       "      <td>-0.244153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.00599499</td>\n",
       "      <td>-0.29975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.00630455</td>\n",
       "      <td>-0.0630455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.0071464</td>\n",
       "      <td>-0.300149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.00251476</td>\n",
       "      <td>-0.0855019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.00518641</td>\n",
       "      <td>-0.243761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.00502089</td>\n",
       "      <td>-0.125522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.00340405</td>\n",
       "      <td>-0.0748891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.00522556</td>\n",
       "      <td>-0.083609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.00706456</td>\n",
       "      <td>-0.127162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.00646052</td>\n",
       "      <td>-0.167974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.00761367</td>\n",
       "      <td>-0.220796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.00552987</td>\n",
       "      <td>-0.154836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.00455294</td>\n",
       "      <td>-0.0637412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.00801251</td>\n",
       "      <td>-0.304476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.00395511</td>\n",
       "      <td>-0.0474614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.00283426</td>\n",
       "      <td>-0.0453481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.00629008</td>\n",
       "      <td>-0.100641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.00798132</td>\n",
       "      <td>-0.11972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.00677544</td>\n",
       "      <td>-0.128733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.00317363</td>\n",
       "      <td>-0.0729934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.00634169</td>\n",
       "      <td>-0.253667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>-0.0073008</td>\n",
       "      <td>-0.0584064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-0.00478262</td>\n",
       "      <td>-0.0478262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.00373282</td>\n",
       "      <td>-0.0373282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>-0.00233737</td>\n",
       "      <td>-0.0350606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-0.0038063</td>\n",
       "      <td>-0.038063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-0.00684008</td>\n",
       "      <td>-0.0752409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-0.00675018</td>\n",
       "      <td>-0.128253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>-0.00578633</td>\n",
       "      <td>-0.0810086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>-0.00700519</td>\n",
       "      <td>-0.161119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>-0.00385741</td>\n",
       "      <td>-0.0501463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>-0.00290768</td>\n",
       "      <td>-0.0407075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.00596368</td>\n",
       "      <td>-0.0596368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.00474496</td>\n",
       "      <td>-0.0948992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.00352946</td>\n",
       "      <td>-0.045883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.00500979</td>\n",
       "      <td>-0.0500979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.00648172</td>\n",
       "      <td>-0.123153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.00292495</td>\n",
       "      <td>-0.0292495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.00682084</td>\n",
       "      <td>-0.0818501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-0.00737547</td>\n",
       "      <td>-0.118007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-0.00611527</td>\n",
       "      <td>-0.0489221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-0.00472138</td>\n",
       "      <td>-0.0755421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-0.00474483</td>\n",
       "      <td>-0.056938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-0.00754149</td>\n",
       "      <td>-0.180996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.00465086</td>\n",
       "      <td>-0.0744138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-0.00624671</td>\n",
       "      <td>-0.0624671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-0.00273403</td>\n",
       "      <td>-0.0300744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>-0.00806425</td>\n",
       "      <td>-0.0967709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.00739334</td>\n",
       "      <td>-0.0739334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-0.00660252</td>\n",
       "      <td>-0.125448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.00663083</td>\n",
       "      <td>-0.0928316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg_w2v_score sum_w2v_score\n",
       "0      -0.0029501    -0.0619521\n",
       "1     -0.00621081     -0.068319\n",
       "2      -0.0032949    -0.0461286\n",
       "3     -0.00535927    -0.0535927\n",
       "4     -0.00718453     -0.114952\n",
       "5     -0.00606689    -0.0364014\n",
       "6     -0.00656565     -0.229798\n",
       "7      -0.0042346    -0.0804575\n",
       "8     -0.00542563     -0.244153\n",
       "9     -0.00599499      -0.29975\n",
       "10    -0.00630455    -0.0630455\n",
       "11     -0.0071464     -0.300149\n",
       "12    -0.00251476    -0.0855019\n",
       "13    -0.00518641     -0.243761\n",
       "14    -0.00502089     -0.125522\n",
       "15    -0.00340405    -0.0748891\n",
       "16    -0.00522556     -0.083609\n",
       "17    -0.00706456     -0.127162\n",
       "18    -0.00646052     -0.167974\n",
       "19    -0.00761367     -0.220796\n",
       "20    -0.00552987     -0.154836\n",
       "21    -0.00455294    -0.0637412\n",
       "22    -0.00801251     -0.304476\n",
       "23    -0.00395511    -0.0474614\n",
       "24    -0.00283426    -0.0453481\n",
       "25    -0.00629008     -0.100641\n",
       "26    -0.00798132      -0.11972\n",
       "27    -0.00677544     -0.128733\n",
       "28    -0.00317363    -0.0729934\n",
       "29    -0.00634169     -0.253667\n",
       "..            ...           ...\n",
       "284    -0.0073008    -0.0584064\n",
       "285   -0.00478262    -0.0478262\n",
       "286   -0.00373282    -0.0373282\n",
       "287   -0.00233737    -0.0350606\n",
       "288    -0.0038063     -0.038063\n",
       "289   -0.00684008    -0.0752409\n",
       "290   -0.00675018     -0.128253\n",
       "291   -0.00578633    -0.0810086\n",
       "292   -0.00700519     -0.161119\n",
       "293   -0.00385741    -0.0501463\n",
       "294   -0.00290768    -0.0407075\n",
       "295   -0.00596368    -0.0596368\n",
       "296   -0.00474496    -0.0948992\n",
       "297   -0.00352946     -0.045883\n",
       "298   -0.00500979    -0.0500979\n",
       "299   -0.00648172     -0.123153\n",
       "300   -0.00292495    -0.0292495\n",
       "301   -0.00682084    -0.0818501\n",
       "302   -0.00737547     -0.118007\n",
       "303   -0.00611527    -0.0489221\n",
       "304   -0.00472138    -0.0755421\n",
       "305   -0.00474483     -0.056938\n",
       "306   -0.00754149     -0.180996\n",
       "307   -0.00465086    -0.0744138\n",
       "308   -0.00624671    -0.0624671\n",
       "309   -0.00273403    -0.0300744\n",
       "310   -0.00806425    -0.0967709\n",
       "311   -0.00739334    -0.0739334\n",
       "312   -0.00660252     -0.125448\n",
       "313   -0.00663083    -0.0928316\n",
       "\n",
       "[471 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
